services:
  sshd:
    image: docker.io/panubo/sshd:latest
    hostname: sshd
    environment:
      SSH_ENABLE_ROOT: "true"
      SSH_ENABLE_PASSWORD_AUTH: "true"
    command: sh -c 'echo "PermitRootLogin yes" >> /etc/ssh/sshd_config && echo "LogLevel DEBUG3" >> /etc/ssh/sshd_config && echo "root:JFzMHfVpvTgEd74XXPq6wARA2Qg3AutJ" | chpasswd && /usr/sbin/sshd -D -e -f /etc/ssh/sshd_config'

#  ftp:
#    image: docker.io/fauria/vsftpd:latest
#    hostname: ftp
#    environment:
#      FTP_USER: test_backup
#      FTP_PASS: test_backup
#      PASV_ENABLE: "YES"
#      PASV_ADDRESS: "ftp"
#      PASV_ADDR_RESOLVE: "YES"
#      PASV_MIN_PORT: 21100
#      PASV_MAX_PORT: 21110

  ftp:
    image: docker.io/iradu/proftpd:latest
    hostname: ftp
    environment:
      FTP_USER_NAME: "test_backup"
      FTP_USER_PASS: "test_backup"
      FTP_MASQUERADEADDRESS: "yes"
      FTP_PASSIVE_PORTS: "21100 31100"
      FTP_MAX_CONNECTIONS: 255

  minio:
    image: docker.io/bitnami/minio:${MINIO_VERSION:-latest}
    hostname: minio
    environment:
      MINIO_DEFAULT_BUCKETS: 'clickhouse'
      MINIO_ROOT_USER: access_key
      MINIO_ROOT_PASSWORD: it_is_my_super_secret_key
      MINIO_SCHEME: https
      BITNAMI_DEBUG: "true"
    healthcheck:
      test: ls -la /bitnami/minio/data/clickhouse/ && curl -skL https://localhost:9000/
      interval: 1s
      retries: 60
    volumes:
      - ./minio_nodelete.sh:/bin/minio_nodelete.sh
      # go install github.com/minio/certgen@latest
      # ~/go/bin/certgen -host "127.0.0.1,localhost,minio"
      - ./minio.crt:/certs/CAs/public.crt
      - ./minio.crt:/certs/public.crt
      - ./minio.key:/certs/private.key

  gcs:
    image: fsouza/fake-gcs-server:latest
    hostname: gcs
    entrypoint:
      - /bin/sh
    command:
      - -c
      - "mkdir -p /data/altinity-qa-test && mkdir -p /data/${QA_GCS_OVER_S3_BUCKET} && fake-gcs-server -data /data -scheme http -port 8080 -public-host gcs:8080"
    environment:
      QA_GCS_OVER_S3_BUCKET: "${QA_GCS_OVER_S3_BUCKET}"
    healthcheck:
      test: nc 127.0.0.1 8080 -z
      interval: 1s
      retries: 30

  azure:
    # todo need to blobKeepAliveTimeout, and merge https://github.com/Azure/Azurite/pull/2454
    # image: mcr.microsoft.com/azure-storage/azurite:latest
    image: docker.io/clickhousepro/azurite:latest
    hostname: devstoreaccount1.blob.azure
    healthcheck:
      test: nc 127.0.0.1 10000 -z
      interval: 1s
      retries: 30
    # command: [ "azurite", "--debug", "/data/debug.log" , "-l", "/data", "--blobHost", "0.0.0.0" ]
    command: [ "azurite", "--debug", "/data/debug.log" , "-l", "/data", "--blobHost", "0.0.0.0","--blobKeepAliveTimeout", "600000" ]
    volumes:
      - azure:/data
#    environment:
#      - AZURITE_DB="mysql://root:root@mysql:3306/azurite_blob"

   #  azure_init:
   #    image: mcr.microsoft.com/azure-cli:latest
   #    command:
   #      - /bin/sh
   #      - -xc
   #      - |
   #        az storage container create --debug --name azure-backup-disk &&
   #        az storage container create --debug --name azure-disk
   #    depends_on:
   #      azure:
   #        condition: service_healthy
   #    environment:
   #      # https://github.com/Azure/Azurite/blob/main/README.md#usage-with-azure-storage-sdks-or-tools
   #      AZURE_STORAGE_CONNECTION_STRING: DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://azure:10000/devstoreaccount1;

  mysql:
    image: docker.io/mysql:${MYSQL_VERSION:-latest}
    command: --gtid_mode=on --enforce_gtid_consistency=ON
    hostname: mysql
    environment:
      MYSQL_ROOT_PASSWORD: "root"
    ports:
      - "3306"
    volumes:
      - mysql:/var/lib/mysql
    healthcheck:
      test: mysqladmin -p=root ping -h localhost
      timeout: 10s
      interval: 1s
      retries: 100

  pgsql:
    image: docker.io/postgres:${PGSQL_VERSION:-latest}
    hostname: pgsql
    environment:
      POSTGRES_USER: "root"
      POSTGRES_PASSWORD: "root"
      # to allow connection from clickhouse 21.3
      POSTGRES_HOST_AUTH_METHOD: "md5"
    ports:
      - "5432"
    command: [ "postgres", "-c", "wal_level=logical" ]
    healthcheck:
      test: pg_isready
      timeout: 10s
      interval: 1s
      retries: 60
    volumes:
      - pgsql:/var/lib/postgresql

  zookeeper:
    image: docker.io/clickhouse/clickhouse-keeper:${CLICKHOUSE_KEEPER_VERSION:-latest-alpine}
    hostname: zookeeper
    volumes:
      - ./clickhouse-keeper.xml:/etc/clickhouse-keeper/conf.d/clickhouse-keeper.xml
      - /var/lib/clickhouse
      - /var/lib/clickhouse-keeper
    environment:
      - CLICKHOUSE_UID=0
      - CLICKHOUSE_GID=0
    healthcheck:
      test: bash -c 'if [[ "$$(echo 'ruok' | nc 127.0.0.1 2181)" == "imok" ]]; then exit 0; else exit 1; fi'
      interval: 1s
      timeout: 2s
      retries: 10
      start_period: 1s


  clickhouse-backup:
    image: docker.io/${CLICKHOUSE_IMAGE:-yandex/clickhouse-server}:${CLICKHOUSE_VERSION:-19.17}
    hostname: clickhouse-backup
    user: root
    entrypoint:
      - /bin/bash
      - -xce
      - sleep infinity
    healthcheck:
      test: bash -c "exit 0"
      interval: 1s
      timeout: 1s
      retries: 5
      start_period: 1s
    environment:
      CLICKHOUSE_VERSION: ${CLICKHOUSE_VERSION:-19.17}
      TZ: UTC
      LOG_LEVEL: "${LOG_LEVEL:-info}"
      S3_DEBUG: "${S3_DEBUG:-false}"
      GCS_DEBUG: "${GCS_DEBUG:-false}"
      FTP_DEBUG: "${FTP_DEBUG:-false}"
      SFTP_DEBUG: "${SFTP_DEBUG:-false}"
      AZBLOB_DEBUG: "${AZBLOB_DEBUG:-false}"
      CLICKHOUSE_DEBUG: "${CLICKHOUSE_DEBUG:-false}"
      GOCOVERDIR: "/tmp/_coverage_/"
# FIPS
      QA_AWS_ACCESS_KEY: ${QA_AWS_ACCESS_KEY}
      QA_AWS_SECRET_KEY: ${QA_AWS_SECRET_KEY}
      QA_AWS_BUCKET: ${QA_AWS_BUCKET}
      QA_AWS_REGION: ${QA_AWS_REGION}
# GCS over S3 embedded backups
      QA_GCS_OVER_S3_ACCESS_KEY: "${QA_GCS_OVER_S3_ACCESS_KEY}"
      QA_GCS_OVER_S3_SECRET_KEY: "${QA_GCS_OVER_S3_SECRET_KEY}"
      QA_GCS_OVER_S3_BUCKET: "${QA_GCS_OVER_S3_BUCKET}"
# https://github.com/Altinity/clickhouse-backup/issues/691:
      AWS_ACCESS_KEY_ID: access_key
      AWS_SECRET_ACCESS_KEY: it_is_my_super_secret_key
    volumes_from:
      - clickhouse
    ports:
      - "7171"
# for delve debugger
      - "40001"
    depends_on:
      clickhouse:
        condition: service_healthy

  clickhouse:
    image: docker.io/${CLICKHOUSE_IMAGE:-yandex/clickhouse-server}:${CLICKHOUSE_VERSION:-19.17}
    hostname: clickhouse
    restart: always
    user: root
    environment:
      CLICKHOUSE_VERSION: ${CLICKHOUSE_VERSION:-19.17}
      CLICKHOUSE_ALWAYS_RUN_INITDB_SCRIPTS: "true"
      TZ: UTC
      LOG_LEVEL: "${LOG_LEVEL:-info}"
      S3_DEBUG: "${S3_DEBUG:-false}"
      GCS_DEBUG: "${GCS_DEBUG:-false}"
      FTP_DEBUG: "${FTP_DEBUG:-false}"
      SFTP_DEBUG: "${SFTP_DEBUG:-false}"
      AZBLOB_DEBUG: "${AZBLOB_DEBUG:-false}"
      CLICKHOUSE_DEBUG: "${CLICKHOUSE_DEBUG:-false}"
      GOCOVERDIR: "/tmp/_coverage_/"
# FIPS
      QA_AWS_ACCESS_KEY: ${QA_AWS_ACCESS_KEY}
      QA_AWS_SECRET_KEY: ${QA_AWS_SECRET_KEY}
      QA_AWS_BUCKET: ${QA_AWS_BUCKET}
      QA_AWS_REGION: ${QA_AWS_REGION}
# https://github.com/Altinity/clickhouse-backup/issues/691:
      AWS_ACCESS_KEY_ID: access_key
      AWS_SECRET_ACCESS_KEY: it_is_my_super_secret_key
#     GCS over S3 object disk
      QA_GCS_OVER_S3_ACCESS_KEY: "${QA_GCS_OVER_S3_ACCESS_KEY}"
      QA_GCS_OVER_S3_SECRET_KEY: "${QA_GCS_OVER_S3_SECRET_KEY}"
      QA_GCS_OVER_S3_BUCKET: "${QA_GCS_OVER_S3_BUCKET}"
      AWS_EC2_METADATA_DISABLED: "true"

    # to avoid backward incompatibility ;(
    # https://t.me/clickhouse_ru/359960
    # https://t.me/clickhouse_ru/359968
    # https://t.me/clickhouse_ru/362378
    entrypoint:
      - "/custom_entrypoint.sh"
    volumes:
# clickhouse-backup related files requires for some tests
      - ${CLICKHOUSE_BACKUP_BIN:-../../clickhouse-backup/clickhouse-backup-race}:/usr/bin/clickhouse-backup
      - ${CLICKHOUSE_BACKUP_BIN_FIPS:-../../clickhouse-backup/clickhouse-backup-race-fips}:/usr/bin/clickhouse-backup-fips
      - ./credentials.json:/etc/clickhouse-backup/credentials.json
      - ./config-azblob.yml:/etc/clickhouse-backup/config-azblob.yml
      - ./config-azblob-embedded.yml:/etc/clickhouse-backup/config-azblob-embedded.yml
      - ./config-azblob-embedded-url.yml:/etc/clickhouse-backup/config-azblob-embedded-url.yml
      - ./config-custom-kopia.yml:/etc/clickhouse-backup/config-custom-kopia.yml
      - ./config-custom-restic.yml:/etc/clickhouse-backup/config-custom-restic.yml
      - ./config-custom-rsync.yml:/etc/clickhouse-backup/config-custom-rsync.yml
      - ./config-database-mapping.yml:/etc/clickhouse-backup/config-database-mapping.yml
      - ./config-ftp.yaml:/etc/clickhouse-backup/config-ftp.yaml
      - ./config-ftp-old.yaml:/etc/clickhouse-backup/config-ftp-old.yaml
      - ./config-gcs.yml:/etc/clickhouse-backup/config-gcs.yml
      - ./config-gcs-embedded-url.yml:/etc/clickhouse-backup/config-gcs-embedded-url.yml.template
      - ./config-gcs-custom-endpoint.yml:/etc/clickhouse-backup/config-gcs-custom-endpoint.yml
      - ./config-s3.yml:/etc/clickhouse-backup/config-s3.yml
      - ./config-s3-embedded.yml:/etc/clickhouse-backup/config-s3-embedded.yml
      - ./config-s3-embedded-url.yml:/etc/clickhouse-backup/config-s3-embedded-url.yml
      - ./config-s3-embedded-local.yml:/etc/clickhouse-backup/config-s3-embedded-local.yml
      - ./config-s3-fips.yml:/etc/clickhouse-backup/config-s3-fips.yml.template
      - ./config-s3-nodelete.yml:/etc/clickhouse-backup/config-s3-nodelete.yml
      - ./config-s3-plain-embedded.yml:/etc/clickhouse-backup/config-s3-plain-embedded.yml
      - ./config-sftp-auth-key.yaml:/etc/clickhouse-backup/config-sftp-auth-key.yaml
      - ./config-sftp-auth-password.yaml:/etc/clickhouse-backup/config-sftp-auth-password.yaml
      - ./_coverage_/:/tmp/_coverage_/
# for local debug
      - ./install_delve.sh:/tmp/install_delve.sh
# clickhouse configuration
      - ./custom_entrypoint.sh:/custom_entrypoint.sh
      - ./dynamic_settings.sh:/docker-entrypoint-initdb.d/dynamic_settings.sh
      - ./enable-access_management.xml:/etc/clickhouse-server/users.d/enable-access_management.xml
      - ./backup-user.xml:/etc/clickhouse-server/users.d/backup-user.xml
      - ./server.crt:/etc/clickhouse-server/server.crt
      - ./server.key:/etc/clickhouse-server/server.key
      - ./dhparam.pem:/etc/clickhouse-server/dhparam.pem
      - ./ssl.xml:/etc/clickhouse-server/config.d/ssl.xml
      - ./clickhouse-config.xml:/etc/clickhouse-server/config.d/clickhouse-config.xml
      - /var/lib/clickhouse
      - /hdd1_data
      - /hdd2_data
      - /hdd3_data
# uncomment only when you need clickhouse logs
#      - ./clickhouse-server.log:/var/log/clickhouse-server/clickhouse-server.log
#      - ./clickhouse-server.err.log:/var/log/clickhouse-server/clickhouse-server.err.log
    ports:
      - "8123"
      - "9000"
# for delve debugger
      - "40002"
    links:
      - zookeeper
      - minio
      - sshd
      - mysql
      - pgsql
      - ftp
      - azure
      - gcs
    healthcheck:
      test: clickhouse client -q "SELECT 1"
      interval: 1s
      timeout: 2s
      retries: 60
      start_period: 1s
    depends_on:
      mysql:
        condition: service_healthy
      pgsql:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
      minio:
        condition: service_healthy
      azure:
        condition: service_healthy
      gcs:
        condition: service_healthy
#      azure_init:
#        condition: service_completed_successfully

  all_services_ready:
    image: hello-world
    depends_on:
      clickhouse-backup:
        condition: service_healthy

volumes:
  mysql:
    driver: local
    driver_opts:
      device: tmpfs
      type: tmpfs
      o: size=250m
  pgsql:
    driver: local
    driver_opts:
      device: tmpfs
      type: tmpfs
      o: size=60m
  azure:
    driver: local
    driver_opts:
      device: tmpfs
      type: tmpfs
      o: size=60m
